{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b33f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Data Augmentation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826e5a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data berhasil dimuat: 68 baris, 9 kolom\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_excel(\"Book1.xlsx\")\n",
    "    print(f\"✅ Data berhasil dimuat: {len(df)} baris, {len(df.columns)} kolom\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File tidak ditemukan\")\n",
    "\n",
    "    np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c274979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data awal: 68 baris\n",
      "Columns: ['Tanggal', 'Jam_Mulai', 'Durasi_Menit', 'Elevasi', 'Jarak', 'Kec_Rata_Rata', 'Kec_Maksimal', 'Curah_Hujan', 'Jam_Tidur']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data awal: {len(df)} baris\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f309844",
   "metadata": {},
   "source": [
    "</H1> DATA CLEANING DAN FEATURE ENGINEERING </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042a66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomali curah hujan ditemukan: 888.0 mm\n",
      "Diganti dengan median: 0.05 mm\n"
     ]
    }
   ],
   "source": [
    "# Handling anomali curah hujan (888.0 mm)\n",
    "print(f\"\\nAnomali curah hujan ditemukan: {df['Curah_Hujan'].max()} mm\")\n",
    "df.loc[df['Curah_Hujan'] == 888.0, 'Curah_Hujan'] = df['Curah_Hujan'].median()\n",
    "print(f\"Diganti dengan median: {df['Curah_Hujan'].median()} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1fac3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime features\n",
    "df['Tanggal'] = pd.to_datetime(df['Tanggal'], format='%d/%m/%Y')\n",
    "df['Jam_Mulai_Datetime'] = pd.to_datetime(df['Jam_Mulai'], format='%H:%M:%S').dt.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4249fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based features\n",
    "df['Hour'] = df['Jam_Mulai'].apply(lambda t: t.hour)\n",
    "df['Is_Morning'] = (df['Hour'] < 12).astype(int)\n",
    "df['Day_of_Week'] = df['Tanggal'].dt.dayofweek\n",
    "df['Is_Weekend'] = (df['Day_of_Week'].isin([5, 6])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b073703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical features\n",
    "df['Time_Category'] = df['Hour'].apply(lambda x: 'Early_Morning' if x < 8 \n",
    "                                       else 'Morning' if x < 12 \n",
    "                                       else 'Afternoon' if x < 17 \n",
    "                                       else 'Evening')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e02e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elevation categories\n",
    "df['Elevation_Category'] = pd.cut(df['Elevasi'], \n",
    "                                  bins=[0, 150, 250, 400, float('inf')],\n",
    "                                  labels=['Flat', 'Rolling', 'Hilly', 'Mountainous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8521562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rain categories  \n",
    "df['Rain_Category'] = pd.cut(df['Curah_Hujan'],\n",
    "                             bins=[-0.1, 0, 10, 30, float('inf')],\n",
    "                             labels=['No_Rain', 'Light', 'Moderate', 'Heavy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb70e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance categories\n",
    "df['Distance_Category'] = pd.cut(df['Jarak'],\n",
    "                                 bins=[0, 20, 30, 40, float('inf')],\n",
    "                                 labels=['Short', 'Medium', 'Long', 'Ultra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdffd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep quality categories\n",
    "df['Sleep_Quality'] = pd.cut(df['Jam_Tidur'],\n",
    "                             bins=[0, 4, 6, 8, float('inf')],\n",
    "                             labels=['Poor', 'Moderate', 'Good', 'Excellent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e371293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering\n",
    "df['Speed_per_Elevation'] = df['Kec_Rata_Rata'] / (df['Elevasi'] + 1)\n",
    "df['Distance_per_Duration'] = df['Jarak'] / df['Durasi_Menit']\n",
    "df['Elevation_per_Distance'] = df['Elevasi'] / df['Jarak']\n",
    "df['Rest_Factor'] = df['Jam_Tidur'] / df['Durasi_Menit'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "246111d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature engineering completed. New features:\n"
     ]
    }
   ],
   "source": [
    "# Weather impact factor\n",
    "df['Weather_Impact'] = df['Curah_Hujan'].apply(lambda x: 1 if x == 0 else 0.8 if x <= 10 else 0.6 if x <= 30 else 0.4)\n",
    "\n",
    "print(f\"\\nFeature engineering completed. New features:\")\n",
    "new_features = [col for col in df.columns if col not in df.keys()]\n",
    "for feature in new_features:\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc2ce8",
   "metadata": {},
   "source": [
    "</H2> EXPLORATORY DATA ANALYSIS </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c5eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistik Dasar Target Variable (Kec_Rata_Rata):\n",
      "Mean: 21.36 km/h\n",
      "Std: 3.63 km/h\n",
      "Min: 15.20 km/h\n",
      "Max: 38.50 km/h\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"\\nStatistik Dasar Target Variable (Kec_Rata_Rata):\")\n",
    "print(f\"Mean: {df['Kec_Rata_Rata'].mean():.2f} km/h\")\n",
    "print(f\"Std: {df['Kec_Rata_Rata'].std():.2f} km/h\")\n",
    "print(f\"Min: {df['Kec_Rata_Rata'].min():.2f} km/h\")\n",
    "print(f\"Max: {df['Kec_Rata_Rata'].max():.2f} km/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b01eab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Korelasi dengan target variable:\n",
      "  Jarak: 0.435\n",
      "  Elevasi: 0.373\n",
      "  Distance_per_Duration: 0.325\n",
      "  Hour: -0.211\n",
      "  Jam_Tidur: 0.172\n",
      "  Elevation_per_Distance: 0.108\n",
      "  Weather_Impact: -0.085\n",
      "  Rest_Factor: 0.041\n",
      "  Curah_Hujan: 0.036\n",
      "  Day_of_Week: 0.029\n",
      "  Speed_per_Elevation: 0.008\n"
     ]
    }
   ],
   "source": [
    "# Correlation analysis\n",
    "numeric_features = ['Elevasi', 'Jarak', 'Curah_Hujan', 'Jam_Tidur', 'Hour', 'Day_of_Week',\n",
    "                    'Speed_per_Elevation', 'Distance_per_Duration', 'Elevation_per_Distance', \n",
    "                    'Rest_Factor', 'Weather_Impact']\n",
    "\n",
    "print(f\"\\nKorelasi dengan target variable:\")\n",
    "correlations = df[numeric_features + ['Kec_Rata_Rata']].corr()['Kec_Rata_Rata'].sort_values(key=abs, ascending=False)\n",
    "for feature, corr in correlations.items():\n",
    "    if feature != 'Kec_Rata_Rata':\n",
    "        print(f\"  {feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355fbcd",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f14596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "categorical_features = ['Time_Category', 'Elevation_Category', 'Rain_Category', \n",
    "                       'Distance_Category', 'Sleep_Quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51afd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le_encoders = {}\n",
    "for cat_feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{cat_feature}_encoded'] = le.fit_transform(df[cat_feature].astype(str))\n",
    "    le_encoders[cat_feature] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a668b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected: 18\n",
      "Dataset shape: (68, 18)\n"
     ]
    }
   ],
   "source": [
    "# Final feature selection\n",
    "feature_columns = (numeric_features + \n",
    "                  [f'{cat}_encoded' for cat in categorical_features] +\n",
    "                  ['Is_Morning', 'Is_Weekend'])\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df['Kec_Rata_Rata'].copy()\n",
    "\n",
    "print(f\"Features selected: {len(feature_columns)}\")\n",
    "print(f\"Dataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5600b5d",
   "metadata": {},
   "source": [
    "DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4cd82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDataAugmentation:\n",
    "    def __init__(self, X, y, random_state=42):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    def gaussian_noise_augmentation(self, noise_factor=0.1, n_samples=50):\n",
    "        \"\"\" Gaussian noise untuk sample baru\"\"\"\n",
    "        X_noise = []\n",
    "        y_noise = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            idx = np.random.randint(0, len(self.X))\n",
    "            sample = self.X.iloc[idx].values.copy()\n",
    "            target = self.y.iloc[idx]\n",
    "            \n",
    "            # menambah noise untuk fitur kontinu\n",
    "            continuous_features = ['Elevasi', 'Jarak', 'Curah_Hujan', 'Jam_Tidur', 'Hour',\n",
    "                                 'Speed_per_Elevation', 'Distance_per_Duration', \n",
    "                                 'Elevation_per_Distance', 'Rest_Factor', 'Weather_Impact']\n",
    "            \n",
    "            for i, feature in enumerate(self.X.columns):\n",
    "                if feature in continuous_features:\n",
    "                    noise = np.random.normal(0, noise_factor * np.std(self.X[feature]))\n",
    "                    sample[i] += noise\n",
    "                    sample[i] = max(0, sample[i])  # memastikan non-negative values\n",
    "            \n",
    "            X_noise.append(sample)\n",
    "            # memastikan target\n",
    "            y_adjustment = np.random.normal(0, 0.5)\n",
    "            y_noise.append(max(10, target + y_adjustment))  # Min speed 10 km/h\n",
    "        \n",
    "        return np.array(X_noise), np.array(y_noise)\n",
    "    \n",
    "    def interpolation_augmentation(self, n_samples=30):\n",
    "        \"\"\"Create new samples by interpolating between existing ones\"\"\"\n",
    "        X_interp = []\n",
    "        y_interp = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            idx1, idx2 = np.random.choice(len(self.X), 2, replace=False)\n",
    "            alpha = np.random.uniform(0.2, 0.8)\n",
    "            \n",
    "            sample1 = self.X.iloc[idx1].values\n",
    "            sample2 = self.X.iloc[idx2].values\n",
    "            target1 = self.y.iloc[idx1]\n",
    "            target2 = self.y.iloc[idx2]\n",
    "            \n",
    "            # Interpolate\n",
    "            new_sample = alpha * sample1 + (1 - alpha) * sample2\n",
    "            new_target = alpha * target1 + (1 - alpha) * target2\n",
    "            \n",
    "            X_interp.append(new_sample)\n",
    "            y_interp.append(new_target)\n",
    "        \n",
    "        return np.array(X_interp), np.array(y_interp)\n",
    "    \n",
    "    def seasonal_variation_augmentation(self, n_samples=20):\n",
    "        \"\"\" Membuat cariasi berdasarkan musim/pola cuaca\"\"\"\n",
    "        X_seasonal = []\n",
    "        y_seasonal = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            idx = np.random.randint(0, len(self.X))\n",
    "            sample = self.X.iloc[idx].values.copy()\n",
    "            target = self.y.iloc[idx]\n",
    "            \n",
    "            # Simulasi cuaca\n",
    "            weather_variation = np.random.choice(['sunny', 'windy', 'humid'])\n",
    "            \n",
    "            if weather_variation == 'windy':\n",
    "                target *= np.random.uniform(0.95, 1.0)\n",
    "            elif weather_variation == 'humid':\n",
    "                target *= np.random.uniform(0.9, 0.98)\n",
    "            else: \n",
    "                target *= np.random.uniform(1.0, 1.05)\n",
    "            \n",
    "            X_seasonal.append(sample)\n",
    "            y_seasonal.append(target)\n",
    "        \n",
    "        return np.array(X_seasonal), np.array(y_seasonal)\n",
    "\n",
    "# Apply augmentation\n",
    "augmenter = AdvancedDataAugmentation(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5550cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented data\n",
    "X_noise, y_noise = augmenter.gaussian_noise_augmentation(n_samples=60)\n",
    "X_interp, y_interp = augmenter.interpolation_augmentation(n_samples=40)\n",
    "X_seasonal, y_seasonal = augmenter.seasonal_variation_augmentation(n_samples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b6ced8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: 68 samples\n",
      "Augmented data: 198 samples\n",
      "Augmentation ratio: 2.9x\n"
     ]
    }
   ],
   "source": [
    "# Combine all data\n",
    "X_augmented = np.vstack([X.values, X_noise, X_interp, X_seasonal])\n",
    "y_augmented = np.hstack([y.values, y_noise, y_interp, y_seasonal])\n",
    "\n",
    "print(f\"Original data: {len(X)} samples\")\n",
    "print(f\"Augmented data: {len(X_augmented)} samples\")\n",
    "print(f\"Augmentation ratio: {len(X_augmented)/len(X):.1f}x\")\n",
    "\n",
    "# Convert kembali ke df untuk consistency\n",
    "X_augmented = pd.DataFrame(X_augmented, columns=X.columns)\n",
    "y_augmented = pd.Series(y_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee9703",
   "metadata": {},
   "source": [
    "MODEL TRAINING DAN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "355a2667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 158 samples\n",
      "Test set: 40 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_augmented, y_augmented, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e41462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1deb456a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing XGBoost hyperparameters...\n"
     ]
    }
   ],
   "source": [
    "# XGBoost parameter tuning\n",
    "print(\"\\nOptimizing XGBoost hyperparameters...\")\n",
    "\n",
    "# Initial XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c231bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n",
      "Best parameters found:\n",
      "  colsample_bytree: 0.8\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 3\n",
      "  n_estimators: 300\n",
      "  reg_alpha: 0.1\n",
      "  reg_lambda: 0.1\n",
      "  subsample: 0.8\n"
     ]
    }
   ],
   "source": [
    "# GridSearch with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model, param_grid, cv=5, scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters found:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4888d14",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97078345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_train = final_model.predict(X_train_scaled)\n",
    "y_pred_test = final_model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "511ab35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Performance:\n",
      "  MAE: 0.128 km/h\n",
      "  R²: 0.995\n",
      "  RMSE: 0.240 km/h\n",
      "\n",
      "Test Performance:\n",
      "  MAE: 0.993 km/h\n",
      "  R²: 0.800\n",
      "  RMSE: 1.240 km/h\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Performance:\")\n",
    "print(f\"  MAE: {train_mae:.3f} km/h\")\n",
    "print(f\"  R²: {train_r2:.3f}\")\n",
    "print(f\"  RMSE: {train_rmse:.3f} km/h\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  MAE: {test_mae:.3f} km/h\")\n",
    "print(f\"  R²: {test_r2:.3f}\")\n",
    "print(f\"  RMSE: {test_rmse:.3f} km/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23dd3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation MAE: 1.078 ± 0.318\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "cv_scores = cross_val_score(final_model, X_train_scaled, y_train, \n",
    "                           cv=5, scoring='neg_mean_absolute_error')\n",
    "print(f\"\\nCross-validation MAE: {-cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1659cc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "  1. Jarak: 0.182\n",
      "  2. Is_Morning: 0.137\n",
      "  3. Elevasi: 0.129\n",
      "  4. Speed_per_Elevation: 0.111\n",
      "  5. Elevation_per_Distance: 0.094\n",
      "  6. Distance_per_Duration: 0.084\n",
      "  7. Rain_Category_encoded: 0.057\n",
      "  8. Sleep_Quality_encoded: 0.046\n",
      "  9. Weather_Impact: 0.027\n",
      "  10. Elevation_Category_encoded: 0.023\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
    "    print(f\"  {i+1}. {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103ffae",
   "metadata": {},
   "source": [
    "MODEL PERSISTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1cf1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create model package\n",
    "model_package = {\n",
    "    'model': final_model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': le_encoders,\n",
    "    'feature_columns': feature_columns,\n",
    "    'feature_importance': feature_importance,\n",
    "    'model_performance': {\n",
    "        'test_mae': test_mae,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'cv_mae_mean': -cv_scores.mean(),\n",
    "        'cv_mae_std': cv_scores.std()\n",
    "    },\n",
    "    'training_info': {\n",
    "        'original_samples': len(X),\n",
    "        'augmented_samples': len(X_augmented),\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90eab344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'cycling_speed_prediction_model_v2.joblib'\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(model_package, 'cycling_speed_prediction_model_v2.joblib')\n",
    "print(\"Model saved as 'cycling_speed_prediction_model_v2.joblib'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
